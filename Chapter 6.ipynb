{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "tf",
   "display_name": "tf"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "def relu(x):\n",
    "    return ( x > 0) * x\n",
    "\n",
    "def reludiv(output):\n",
    "    return output > 0\n",
    "\n",
    "alpha = 0.2\n",
    "hidden_size = 4\n",
    "\n",
    "streetlights = np.array( [ [ 1, 0, 1 ],\n",
    "                           [ 0, 1, 1 ],\n",
    "                           [ 0, 0, 1 ],\n",
    "                           [ 1, 1, 1 ] ] )\n",
    "\n",
    "walk_vs_stop = np.array( [ 1, 1, 0, 0 ] ).T\n",
    "\n",
    "weights_0_1 = 2 * np.random.random((3, hidden_size)) - 1\n",
    "weights_1_2 = 2 * np.random.randpm((hidden_size, 1)) - 1\n",
    "\n",
    "for iteration in range(60):\n",
    "    layer_2_error = 0\n",
    "    for i in range(len(streetlights)):\n",
    "        \n",
    "\n",
    "layer_0 = streetlights[0]\n",
    "layer_1 = relu(np.dot(layer_0,weights_0_1))\n",
    "layer_2 = np.dot(layer_1,weights_1_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Error: 0.6342311598444467\nError: 0.35838407676317513\nError: 0.0830183113303298\nError: 0.006467054957103705\nError: 0.0003292669000750734\nError: 1.5055622665134859e-05\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "def relu(x):\n",
    "    return ( x > 0) * x\n",
    "\n",
    "def relu2deriv(output):\n",
    "    return output>0\n",
    "\n",
    "alpha = 0.2\n",
    "hidden_size = 4\n",
    "\n",
    "streetlights = np.array( [ [ 1, 0, 1 ],\n",
    "                           [ 0, 1, 1 ],\n",
    "                           [ 0, 0, 1 ],\n",
    "                           [ 1, 1, 1 ] ] )\n",
    "\n",
    "walk_vs_stop = np.array( [ 1, 1, 0, 0 ] ).T\n",
    "\n",
    "weights_0_1 = 2 * np.random.random((3, hidden_size)) - 1\n",
    "weights_1_2 = 2 * np.random.random((hidden_size, 1)) - 1\n",
    "\n",
    "for iteration in range(60):\n",
    "    layer_2_error = 0\n",
    "    for i in range(len(streetlights)):\n",
    "        layer_0 = streetlights[i:i+1]\n",
    "        layer_1 = relu(np.dot(layer_0,weights_0_1))\n",
    "        layer_2 = np.dot(layer_1,weights_1_2)\n",
    "\n",
    "        layer_2_error += np.sum((layer_2 - walk_vs_stop[i:i+1]) ** 2)\n",
    "\n",
    "        layer_2_delta = (layer_2 - walk_vs_stop[i:i+1])\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T)*relu2deriv(layer_1)\n",
    "\n",
    "        weights_1_2 -= alpha * layer_1.T.dot(layer_2_delta)\n",
    "        weights_0_1 -= alpha * layer_0.T.dot(layer_1_delta)\n",
    "        \n",
    "\n",
    "    if(iteration % 10 == 9):\n",
    "        print(\"Error: \" + str(layer_2_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Error: 0.03999999999999998 Prediction: -0.19999999999999996\nError: 0.025599999999999973 Prediction: -0.15999999999999992\nError: 0.01638399999999997 Prediction: -0.1279999999999999\nError: 0.010485759999999964 Prediction: -0.10239999999999982\nError: 0.006710886399999962 Prediction: -0.08191999999999977\nError: 0.004294967295999976 Prediction: -0.06553599999999982\nError: 0.002748779069439994 Prediction: -0.05242879999999994\nError: 0.0017592186044416036 Prediction: -0.04194304000000004\nError: 0.0011258999068426293 Prediction: -0.03355443200000008\nError: 0.0007205759403792803 Prediction: -0.02684354560000002\nError: 0.0004611686018427356 Prediction: -0.021474836479999926\nError: 0.0002951479051793508 Prediction: -0.01717986918399994\nError: 0.00018889465931478573 Prediction: -0.013743895347199997\nError: 0.00012089258196146188 Prediction: -0.010995116277759953\nError: 7.737125245533561e-05 Prediction: -0.008796093022207963\nError: 4.951760157141604e-05 Prediction: -0.007036874417766459\nError: 3.169126500570676e-05 Prediction: -0.0056294995342132115\nError: 2.028240960365233e-05 Prediction: -0.004503599627370569\nError: 1.298074214633813e-05 Prediction: -0.003602879701896544\nError: 8.307674973656916e-06 Prediction: -0.002882303761517324\nPrediction: -0.0023058430092137705\nPrediction: -0.1209223372036855\nPrediction: -0.4888301034833169\nPrediction: 0.7512228033816979\nPrediction: 0.20190057990904375\nPrediction: 0.2886959509940853\n Error: 2.2776252030528275\n\nPrediction: 0.23095676079526822\nPrediction: 0.3095551927482997\nPrediction: -0.31818022191782463\nPrediction: 1.0514522876696315\nPrediction: 0.46917171885649595\nPrediction: 0.3284202821335913\n Error: 1.02357959452247\n\nPrediction: 0.26273622570687305\nPrediction: 0.5162217243011503\nPrediction: -0.24916242359281637\nPrediction: 1.1456228535284017\nPrediction: 0.6087690510945215\nPrediction: 0.2934815746795325\n Error: 0.6255527631951046\n\nPrediction: 0.23478525974362602\nPrediction: 0.6341885574333014\nPrediction: -0.21593091088147304\nPrediction: 1.1692841999678323\nPrediction: 0.6950870970412221\nPrediction: 0.24263689344202927\n Error: 0.4160699687884104\n\nPrediction: 0.19410951475362342\nPrediction: 0.7123950368134124\nPrediction: -0.19568909399513768\nPrediction: 1.1692221409669388\nPrediction: 0.7556405106568559\nPrediction: 0.19420853826199794\n Error: 0.2847539894354219\n\nPrediction: 0.15536683060959836\nPrediction: 0.7695548716383251\nPrediction: -0.1805194738089955\nPrediction: 1.1615532958244013\nPrediction: 0.8013851855266794\nPrediction: 0.1529407469871975\n Error: 0.19876927367257707\n\nPrediction: 0.12235259758975803\nPrediction: 0.813578813963648\nPrediction: -0.16764859041726432\nPrediction: 1.151800697305351\nPrediction: 0.8372677707515747\nPrediction: 0.11920213918094039\n Error: 0.1415634467329222\n\nPrediction: 0.0953617113447523\nPrediction: 0.8483578315486905\nPrediction: -0.15608274638866876\nPrediction: 1.1418308721874209\nPrediction: 0.8659283654403351\nPrediction: 0.09210284957828199\n Error: 0.10302516136282915\n\nPrediction: 0.07368227966262561\nPrediction: 0.8761641794281774\nPrediction: -0.14544532637948598\nPrediction: 1.1322506083472592\nPrediction: 0.8890257545110386\nPrediction: 0.07052124130467569\n Error: 0.07669748380382993\n\nPrediction: 0.05641699304374054\nPrediction: 0.8985267801739892\nPrediction: -0.1355749314796077\nPrediction: 1.1232347651843533\nPrediction: 0.9077319642502817\nPrediction: 0.05341826010365544\n Error: 0.05841396177531147\n\nPrediction: 0.04273460808292437\nPrediction: 0.9165702845815674\nPrediction: -0.12638642655192514\nPrediction: 1.1148119548805544\nPrediction: 0.9229324793443356\nPrediction: 0.039913661752830856\n Error: 0.04547458107739206\n\nPrediction: 0.03193092940226468\nPrediction: 0.931161524359959\nPrediction: -0.11782283887072709\nPrediction: 1.106962933331583\nPrediction: 0.9353189167087234\nPrediction: 0.029286396635699624\n Error: 0.03612294601102748\n\nPrediction: 0.023429117308559705\nPrediction: 0.9429835819725526\nPrediction: -0.10983864957936619\nPrediction: 1.0996543157649374\nPrediction: 0.9454398673829914\nPrediction: 0.02095395071624251\n Error: 0.02921118317633983\n\nPrediction: 0.016763160572994013\nPrediction: 0.9525801827774695\nPrediction: -0.10239393234289304\nPrediction: 1.0928499819798063\nPrediction: 0.9537335430603038\nPrediction: 0.014448552712945928\n Error: 0.02398462486535704\n\nPrediction: 0.011558842170356742\nPrediction: 0.9603860949599128\nPrediction: -0.0954522405969363\nPrediction: 1.0865148048648543\nPrediction: 0.9605511390546531\nPrediction: 0.009395613421551582\n Error: 0.019943300182076056\n\nPrediction: 0.007516490737241269\nPrediction: 0.9667497008278431\nPrediction: -0.08897979142785699\nPrediction: 1.0806157537399257\nPrediction: 0.966174589057075\nPrediction: 0.005495591996101765\n Error: 0.016752743018487618\n\nPrediction: 0.004396473596881403\nPrediction: 0.9719504646863616\nPrediction: -0.08294509959270584\nPrediction: 1.0751221137099347\nPrediction: 0.9708304589663729\nPrediction: 0.002509173729515299\n Error: 0.01418648500300872\n\nPrediction: 0.0020073389836122307\nPrediction: 0.9762127159017855\nPrediction: -0.07731876976255733\nPrediction: 1.0700054190569526\nPrediction: 0.9747009658862935\nPrediction: 0.00024529617294709316\n Error: 0.012088916446798131\n\nPrediction: 0.00019623693835767175\nPrediction: 0.9797166193979042\nPrediction: -0.07207334653154711\nPrediction: 1.065239304313921\nPrediction: 0.977932769308694\nPrediction: -0.0014484755296031715\n Error: 0.010351248896949493\n\nPrediction: -0.0011587804236825316\nPrediction: 0.9826069410422836\nPrediction: -0.0671831877495537\nPrediction: 1.0607993409151615\nPrediction: 0.98064400342575\nPrediction: -0.0026936681938263263\n Error: 0.008895912295607529\n\nPrediction: -0.002154934555061057\nPrediction: 0.9850000630154887\nPrediction: -0.0626243494353496\nPrediction: 1.056662880845678\nPrediction: 0.9829299091867906\nPrediction: -0.0035870860898773935\n Error: 0.007666388247395584\n\nPrediction: -0.0028696688719019106\nPrediction: 0.9869896028456103\nPrediction: -0.05837447828344459\nPrediction: 1.0528089130061948\nPrediction: 0.9848673475035937\nPrediction: -0.004205764905336445\n Error: 0.006620552071325463\n\nPrediction: -0.003364611924269151\nPrediction: 0.9886509156858355\nPrediction: -0.05441271039170197\nPrediction: 1.0492179328715419\nPrediction: 0.9865184170135304\nPrediction: -0.004610938344490077\n Error: 0.005726284128727537\n\nPrediction: -0.003688750675592066\nPrediction: 0.9900447025128325\nPrediction: -0.05071957569031403\nPrediction: 1.0458718244778547\nPrediction: 0.9879333546837266\nPrediction: -0.004851213586669081\n Error: 0.00495855267118951\n\nPrediction: -0.003880970869335268\nPrediction: 0.9912199021925816\nPrediction: -0.04727690781109847\nPrediction: 1.0427537534315474\nPrediction: 0.9891528618488656\nPrediction: -0.004965113004812567\n Error: 0.004297454250281473\n\nPrediction: -0.003972090403850055\nPrediction: 0.9922160098199589\nPrediction: -0.04406775919915956\nPrediction: 1.0398480696699668\nPrediction: 0.9902099698418897\nPrediction: -0.00498310830334231\n Error: 0.003726880121416521\n\nPrediction: -0.003986486642673842\nPrediction: 0.9930649353681134\nPrediction: -0.041076321272638966\nPrediction: 1.037140218843443\nPrediction: 0.991131536653066\nPrediction: -0.004929248157681733\n Error: 0.003233594351768481\n\nPrediction: -0.003943398526145385\nPrediction: 0.9937924939908355\nPrediction: -0.03828784942572682\nPrediction: 1.034616661340968\nPrediction: 0.9919394478670475\nPrediction: -0.004822460332325515\n Error: 0.002806584803177114\n\nPrediction: -0.0038579682658604064\nPrediction: 0.9944196011534567\nPrediction: -0.03568859265948276\nPrediction: 1.0322647981201623\nPrediction: 0.9926515805646812\nPrediction: -0.00467759314258628\n Error: 0.002436596759981141\n\nPrediction: -0.0037420745140690215\nPrediction: 0.9949632312174104\nPrediction: -0.03326572761809433\nPrediction: 1.0300729026208357\nPrediction: 0.9932825772115706\nPrediction: -0.004506248216511032\n Error: 0.002115790309348945\n\nPrediction: -0.0036049985732088258\nPrediction: 0.9954371864482284\nPrediction: -0.031007296805376384\nPrediction: 1.0280300581411068\nPrediction: 0.9938444672108991\nPrediction: -0.004317446172163543\n Error: 0.0018374828221588261\n\nPrediction: -0.0034539569377308316\nPrediction: 0.9958527140797087\nPrediction: -0.028902150757020767\nPrediction: 1.0261261001383342\nPrediction: 0.994347166311802\nPrediction: -0.004118158541300487\n Error: 0.001595950984329879\n\nPrediction: -0.0032945268330403926\nPrediction: 0.9962190015868758\nPrediction: -0.026939893947585798\nPrediction: 1.024351562986725\nPrediction: 0.9947988780669142\nPrediction: -0.003913732634397749\n Error: 0.0013862753343536526\n\nPrediction: -0.0031309861075182015\nPrediction: 0.9965435743277229\nPrediction: -0.025110834216771842\nPrediction: 1.0226976307818405\nPrediction: 0.9952064167274873\nPrediction: -0.0037082307262265024\n Error: 0.001204215806313845\n\nPrediction: -0.002966584580981202\nPrediction: 0.9968326149127107\nPrediction: -0.02340593550657774\nPrediction: 1.0211560918313483\nPrediction: 0.9955754671145567\nPrediction: -0.0035047006831235746\n Error: 0.001046110409014471\n\nPrediction: -0.002803760546498859\nPrediction: 0.9970912198146076\nPrediction: -0.021816773709008974\nPrediction: 1.0197192965129362\nPrediction: 0.9959107939199998\nPrediction: -0.003305391742346179\n Error: 0.0009087915663393206\n\nPrediction: -0.0026443133938769438\nPrediction: 0.9973236056496222\nPrediction: -0.02033549543274159\nPrediction: 1.0183801182156498\nPrediction: 0.9962164104198419\nPrediction: -0.003111926421903756\n Error: 0.0007895162363265099\n\nPrediction: -0.0024895411375230048\nPrediction: 0.9975332750918161\nPrediction: -0.01895477950625553\nPrediction: 1.017131917111134\nPrediction: 0.9964957146018517\nPrediction: -0.0029254373509864165\n Error: 0.0006859069967278241\n\nPrediction: -0.0023403498807891346\nPrediction: 0.9977231504046589\nPrediction: -0.01766780104421687\nPrediction: 1.0159685065272686\nPrediction: 0.9967515991226952\nPrediction: -0.0027466760583987337\n Error: 0.0005959020137232195\n\nPrediction: -0.0021973408467189856\nPrediction: 0.9978956809886679\nPrediction: -0.016468197913146573\nPrediction: 1.0148841217191535\nPrediction: 0.9969865402384182\nPrediction: -0.0025760993525998453\n Error: 0.0005177123147643951\n\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "weights = np.array([ 0.5, 0.48, -0.7])\n",
    "alpha = 0.1\n",
    "\n",
    "streetlights = np.array( [ [ 1, 0, 1 ],\n",
    "                           [ 0, 1, 1 ],\n",
    "                           [ 0, 0, 1 ],\n",
    "                           [ 1, 1, 1 ],\n",
    "                           [ 0, 1, 1 ],\n",
    "                           [ 1, 0, 1 ] ] )\n",
    "\n",
    "decision = np.array( [ [ 0 ],\n",
    "                       [ 1 ],\n",
    "                       [ 0 ],\n",
    "                       [ 1 ],\n",
    "                       [ 1 ],\n",
    "                       [ 0 ] ] )\n",
    "\n",
    "walk_vs_stop = np.array([ 0, 1, 0, 1, 1, 0 ])\n",
    "\n",
    "\n",
    "#This is the section where we train the neural networkA\n",
    "\n",
    "input = streetlights[0]\n",
    "goal_prediction = walk_vs_stop[0]\n",
    "\n",
    "for iteration in range(20):\n",
    "    prediction = input.dot(weights)\n",
    "    error = (goal_prediction - prediction) ** 2\n",
    "    delta = prediction - goal_prediction\n",
    "    weights = weights - (alpha * (input * delta))\n",
    "\n",
    "    print(\"Error: \" + str(error) + \" Prediction: \" + str(prediction))\n",
    "\n",
    "# Now we're training the network to recognize all the signs\n",
    "        \n",
    "\n",
    "for iteration in range(40):\n",
    "    error_for_all_lights = 0\n",
    "    for row_index in range(len(walk_vs_stop)):\n",
    "        input = streetlights[row_index]\n",
    "        goal_prediction = walk_vs_stop[row_index]\n",
    "\n",
    "        prediction = input.dot(weights)\n",
    "        error = (goal_prediction - prediction) ** 2\n",
    "        error_for_all_lights += error\n",
    "\n",
    "        delta = prediction - goal_prediction\n",
    "        weights = weights - (alpha * (input * delta))\n",
    "\n",
    "        print(\"Prediction: \" + str(prediction))\n",
    "    print(\" Error: \" + str(error_for_all_lights) + \"\\n\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# The section above imports the matrix and vector operations required for this.  I also imported tensorflow because why the fuck not\n",
    "# The section below sets the seed for the random number generator to be used to create the weights matrices.\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "# This section sets the height of the first weights matrix, and the length of the second weights matrix.  It also sets the learning parameter.\n",
    "\n",
    "hidden_size = 4\n",
    "alpha = 0.2\n",
    "\n",
    "# This definition creates the nonlinearity.  It's essentially a Heaviside step function multiplied by x.\n",
    "\n",
    "def relu(x):\n",
    "    return (x > 0) * x\n",
    "\n",
    "# This is the derivative of the previous nonlinear function.  You use this when computing the change required.  Yay, calculus!\n",
    "\n",
    "def relu2deriv(output):\n",
    "    return (output > 0)\n",
    "\n",
    "# These are the weights matrices that connect the three layers:\n",
    "# The first matrix is the width of the input matrix and the height of the hidden layer, or M(ixh)\n",
    "# The second matrix is the width of the hidden layer and the height of the output layer, or M(hxo)\n",
    "# We multiply by two since we need to make up for the face that the relu(x) function is going to halve the signal sent between layers\n",
    "# The negative 1 is just the bias vector\n",
    "# The reason these matrices have these heights and lengths is because we're connecting every node in each layer to each other, so the matrix size reflects this\n",
    "\n",
    "weights_0_1 = 2 * np.random.random((3,hidden_size)) - 1\n",
    "weights_1_2 = 2 * np.random.random((hidden_size,1)) - 1\n",
    "\n",
    "# Here is the pattern that we are given\n",
    "\n",
    "streetlights = np.array( [ [ 1, 0, 1 ],\n",
    "                           [ 0, 1, 1 ],\n",
    "                           [ 0, 0, 1 ],\n",
    "                           [ 1, 1, 1 ] ] )\n",
    "\n",
    "# Here is the output we want our deep network to predict\n",
    "\n",
    "walk_vs_stop = np.array([[ 1, 1, 0, 0]]).T\n",
    "\n",
    "# Iterate a decent amount of times and initialize the output layer's error to 0\n",
    "\n",
    "for iteration in range(60):\n",
    "    layer_2_error = 0\n",
    "\n",
    "# And here is where the fun begins.  The first section says that we're iterating over all of the rows of the input matrix.\n",
    "# We have to pull a slice of the matrix for the input, the first layer is then the dot product of the input and the first weight matrix\n",
    "# In doing so, we also nonlinearize the signals being sent, and ensure that the layer is a proper vector\n",
    "# The second layer is simply the dot product of the first layer and the weights between 1 and 2\n",
    "\n",
    "    for i in range(len(streetlights)):\n",
    "        layer_0 = streetlights[i:i+1]\n",
    "        layer_1 = relu(np.dot(layer_0,weights_0_1))\n",
    "        layer_2 = np.dot(layer_1,weights_1_2)\n",
    "\n",
    "# Here we compute the difference between what the second layer outputs, and what the output *should* be.  We square the error to ensure that it is always positive.\n",
    "\n",
    "        layer_2_error += np.sum((layer_2 - walk_vs_stop[i:i+1]) ** 2)\n",
    "\n",
    "# In this section, we thus compute the \"derivative\" of the error at the point we are at.  If you recognize this from a PID function, GOOD!\n",
    "# Remember, these values are VECTORS, not MATRICES.  This is how much each LAYER needs to change\n",
    "\n",
    "        layer_2_delta = (layer_2 - walk_vs_stop[i:i+1])\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T)*relu2deriv(layer_1)\n",
    "\n",
    "# In this section, we multiply the learning parameter by the slope of the error combined with the errors multiplied earlier.\n",
    "# If you notice, we're transposing the first (layer proper) vector.  This creates the matrix size required to properly change the weights vector.\n",
    "\n",
    "        weights_1_2 -= alpha * layer_1.T.dot(layer_2_delta)\n",
    "        weights_0_1 -= alpha * layer_0.T.dot(layer_1_delta)\n",
    "\n",
    "# This just shows us whats going on with the error.  Totes unnecessary, and totes awesome.\n",
    "\n",
    "    if(iteration % 10 == 9):\n",
    "        print(\"Error: \" + str(layer_2_error))"
   ]
  }
 ]
}